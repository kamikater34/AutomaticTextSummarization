{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"import\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\walter\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\walter\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Walter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Walter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Walter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input(topic):\n",
    "\tarticle = \"\"\n",
    "\tlink = \"https://en.wikipedia.org/wiki/\" + topic.strip() \n",
    "\tpage = requests.get(link)\n",
    "\tcontent = BeautifulSoup(page.content,'html.parser')\n",
    "\tparagraphs = content.find_all('p')\n",
    "\tfor paragraph in paragraphs:\n",
    "\t\tarticle+= paragraph.text+\" \"\n",
    "\treturn article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentences):\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tcleaned_sentences = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence = sentence.lower()\n",
    "\t\tsentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "\t\tsentence = sentence.split()\n",
    "\t\tsentence = [lemmatizer.lemmatize(word) for word in sentence if word not in set(stopwords.words('english'))]\n",
    "\t\tsentence = ' '.join(sentence)\n",
    "\t\tcleaned_sentences.append(sentence)\n",
    "\treturn cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_probability(sentences):\n",
    "\tprobability_dict = {}\n",
    "\twords = word_tokenize('. '.join(sentences))\n",
    "\ttotal_words = len(set(words))\n",
    "\tfor word in words:\n",
    "\t\tif word!='.':\n",
    "\t\t\tif not probability_dict.get(word):\n",
    "\t\t\t\tprobability_dict[word] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tprobability_dict[word] += 1\n",
    "\n",
    "\tfor word,count in probability_dict.items():\n",
    "\t\tprobability_dict[word] = count/total_words \n",
    "\t\n",
    "\treturn probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_probability(probability_dict,word):\n",
    "\tif probability_dict.get(word):\n",
    "\t\tprobability_dict[word] = probability_dict[word]**2\n",
    "\treturn probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_sentence_weights(sentences,probability_dict):\n",
    "\tsentence_weights = {}\n",
    "\tfor index,sentence in enumerate(sentences):\n",
    "\t\tif len(sentence) != 0:\n",
    "\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n",
    "\t\t\taverage_proba /= len(sentence)\n",
    "\t\t\tsentence_weights[index] = average_proba \n",
    "\treturn sentence_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n",
    "\tsummary = \"\"\n",
    "\tcurrent_length = 0\n",
    "\twhile current_length < summary_length :\n",
    "\t\thighest_probability_word = max(probability_dict,key=probability_dict.get)\n",
    "\t\tsentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in set(word_tokenize(sentence))]\n",
    "\t\tsentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n",
    "\t\tsummary += tokenized_article[sentence_list[0][0]] + \"\\n\"\n",
    "\t\tfor word in word_tokenize(cleaned_article[sentence_list[0][0]]):\n",
    "\t\t\tprobability_dict = update_probability(probability_dict,word)\n",
    "\t\tcurrent_length+=1\n",
    "\treturn summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\ttopic = input(\"Enter the title of the wikipedia article to be scraped----->\")\n",
    "\tarticle = _input(topic)\n",
    "\trequired_length = int(input(\"Enter the number of required sentences\"))\n",
    "\ttokenized_article = sent_tokenize(article)\n",
    "\tcleaned_article = clean(tokenized_article) \n",
    "\tprobability_dict = init_probability(cleaned_article)\n",
    "\tsentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n",
    "\tsummary = generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,required_length)\n",
    "\tprint(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the title of the wikipedia article to be scraped----->Babylon\n",
      "Enter the number of required sentences20\n",
      "It has been estimated that Babylon was the largest city in the world c. 1770 – c. 1670 BC, and again c. 612 – c. 320 BC.\n",
      "[18]\n",
      " Ctesias, quoted by Diodorus Siculus and in George Syncellus's Chronographia, claimed to have access to manuscripts from Babylonian archives, which date the founding of Babylon to 2286 BC, under the reign of its first king, Belus.\n",
      "Babylon thus became the capital of the Neo-Babylonian (sometimes and possibly erroneously called the Chaldean) or Caldanian Empire.\n",
      "The Arameans briefly ruled in Babylon during the late 11th century BC.\n",
      "The Assyrian king Tukulti-Ninurta I took the throne of Babylon in 1235 BC.\n",
      "[87][88] On July 5, 2019, the site of Babylon was inscribed as a UNESCO World Heritage Site.\n",
      "Babylon was the capital city of Babylonia, a kingdom in ancient Mesopotamia, between the 18th and 6th centuries BC.\n",
      "[12][21]\n",
      " The so-called Weidner Chronicle (also known as ABC 19) states that Sargon of Akkad (c. 23d century BC in the short chronology) had built Babylon \"in front of Akkad\" (ABC 19:51).\n",
      "Information on the Neo-Babylonian city is available from archaeological excavations and from classical sources.\n",
      "In 2003, he intended the construction of a cable car line over Babylon, but plans were halted by the 2003 invasion of Iraq.\n",
      "In the Hebrew Bible, the name appears as Babel (Hebrew: בָּבֶל‎ Bavel, Tib.\n",
      "A reconstruction of the Ishtar Gate is located in the Pergamon Museum in Berlin.\n",
      "[93]\n",
      " The Book of Revelation in the Christian Bible refers to Babylon many centuries after it ceased to be a major political center.\n",
      "Much of the western half of the city is now beneath the river, and other parts of the site have been mined for commercial building materials.\n",
      "[29][30][31]\n",
      " With the recovery of Babylonian independence, a new era of architectural activity ensued, particularly during the reign of his son Nebuchadnezzar II (604–561 BC).\n",
      "The Persian army conquered the outlying areas of the city while the majority of Babylonians at the city center were unaware of the breach.\n",
      "Babylon was described, perhaps even visited, by a number of classical historians including Ctesias, Herodotus, Quintus Curtius Rufus, Strabo, and Cleitarchus.\n",
      "Both are credited with building the walls of Babylon.\n",
      "[48]\n",
      " Claudius Rich, working for the East India Company in Baghdad, excavated Babylon in 1811–12 and again in 1817.\n",
      "Sumu-la-El, whose dates may be concurrent with those of Sumu-abum, is usually given as the progenitor of the First Babylonian dynasty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
